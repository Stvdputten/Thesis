\chapter{Research Background}  \label{ch:research}

In this chapter, related research will be described. In Section \ref{research:relatedwork}, research conducted on twitter tweets and cyber security are reviewed. In Section \ref{research:featureextraction}, research that has been conducted on extracting and transforming logs to features are described. The last Section \ref{research:buildingblocks} introduces the tools and frameworks used during this research.

\section{Topic modelling in Twitter and NCSA} \label{research:relatedwork}
In this section, we briefly describe and show two researches that have applied topic modelling for different use cases. The descriptions will end with an overview explaining which parts are relevant to our current research. 

\setlength{\parindent}{3ex} A research conducted in 2011 makes use of the numerous amounts of tweets on Twitter. Twitter is an online platform used to send messages about social media and news. Users make posts called tweets that are restricted to 140 characters. The authors are interested in finding news topics from twitter feeds and comparing the topics with traditional news feeds using Latent Dirichlet allocation (LDA), see Chapter \ref{ch:theory} for more explanation.

The authors start with preparing three months\' worth of tweets of users and news feed data from New York Times. The tweets are filtered on stop words and tweets appearing more than 70\% of the time and less than 10 times are removed. The authors recognise that LDA does not perform very well on small tweets. To better fit the data to LDA, the authors aggregate the tweets of each user to a single document. The data is represented in a Bag of Words matrix and fitted to their custom Twitter-LDA model. The authors continue comparing the traditional LDA model and their own model using two human judges. The judges compare the generated topics, which are 10 words long, using a self-created scoring mechanism. The scored is based on each topics distinction and cohesion of words. The final results show that their Twitter-LDA model is better compared to the traditional LDA model. The results of their model showed more informative topics, which could not be extracted from the traditional news source \cite{Zhao2011ComparingModels}. 

\newpage
The second research is published in 2014 with the intention of exploring a big data use case. The National Centre of Supercomputing Applications (NCSA) generates around 4.5 GB of data each day which are collected from security monitoring and system logs. The authors study a new approach of LDA on logs files for intrusion detection, moreover the authors propose that LDA can be used to detect patterns in log events. The model is trained to recognise normal activities and abnormal behaviour patterns found in the probability distribution of the topics over their data set. The resulting trained model is capable of detecting the semantics of the log events, which provide a higher level description of the intention of an invading user \cite{Jingwei2014KnowledgeLDA}. 

In our research we will apply ideas mentioned in both researches. In the first paper we recognise two similarities with our own research. Our data set is similarly structured to tweets, which are short and domain specific. The second paper proposes an approach for using log files to detect intrusion, which is similar to our problem of detecting errors. Furthermore, the evaluation in both researches of their own model is based on human judgements and the semantic pattern recognition quality, which both depend on the distinct nature of the topics using cohesion and distinctiveness. In other words, the quality of our model is based on the topic quality detected after applying LDA.

\section{Feature extraction from logs}\label{research:featureextraction}
Numerous log files are outputted by different servers, e.g. web server logs, system logs, etc. A log includes data that can be numerical or non-numerical data, depending on the format and source of the log. Intel researched log based predictive maintenance back in 2014 \cite{Sipos2014Log-basedMaintenance}. The logs contained 3 types of information which was used for feature extraction. 
The content of each log can be used for feature extraction which depends on the type of data.
Keywords from textual messages were extracted using parsing and transformed to a Bag of Words representation. The numerical values were decoded and event codes for sequential analysis. We apply the same text mining techniques to extract only the textual content from our data and transform the data to a usable state.

\begin{comment}
The data that logs can include can be numerical data and non-numerical data
\setlength{\parindent}{3ex}  When we view the 

\cite{Xu2009DetectingLogs}

By comparing a traditional news source with
Topic modelling is a very popular and high researched field. While topic modelling  
An interesting adaptation of topic modelling can be found in twitter and cyber security. Twitter is a interesting field of research with real time tweets and a huge community. Due to the nature of twitter, analysing the huge stream of tweets can be a challenging and exhaustive task. A model with LDA has been introduced to analyse and detect topics. This model is an interesting way to provide news feeds even quicker then traditional news sites. Cyber security has also been an area which LDA seems applicable to. Through the usage of Big data and combination of LDA, users could be identified through system and network logs. Using the event logs to identify topics, new events of users could be identified as malicious or normal 

\end{comment}

\section{Building blocks} \label{research:buildingblocks}
This section has an oversight of the main tools and packages used. This section might be mentioned or referenced in later parts of this paper. The packages were required for the extraction, loading and transforming the data in a usable form.

\subsection{Packages and libraries}

\begin{enumerate}
    \item \textbf{Hadoop} $($\url{http://hadoop.apache.org/}$)$ \\
    The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. 
    \begin{enumerate}
        \item \textbf{HDFS} $($\url{http://hadoop.apache.org/}$)$ \\
        A distributed file system that provides high-throughput access to application data. Used to store big data.
        \item \textbf{Apache Spark} $($\url{https://spark.apache.org/}$)$\\
        Apache Spark is a fast and general engine for large-scale data processing.
        \item \textbf{Apache Zeppelin} $($\url{https://zeppelin.apache.org/}$)$ \\
         Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more.
    \end{enumerate}
    
    \item \textbf{Scikit-learn} $($\url{http://scikit-learn.org/}$)$ \\
    The Scikit-learn package contains tools for effici\"ent data mining and data analysis with machine learning in Python.
    \begin{enumerate}
        \item \textbf{Pandas} $($\url{http://pandas.pydata.org/}$)$ \\
        Pandas is a library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.
        \item \textbf{Numpy} $($\url{http://www.numpy.org/}$)$ \\
        Numpy is a scientific package with Python for powerful array objects, functions and lots of mathematical capabilities.
        \item \textbf{SciPy} $($\url{http://www.numpy.org/}$)$ \\
        SciPy is a scientific package for mathematical, scientific and engineering tools used in Python. 
        \item \textbf{Matplotlib} $($\url{http://matplotlib.org/}$)$ \\
        Matplotlib is a 2D Python plotting library very similar to MATLAB.
        \item \textbf{Seaborn} $($\url{https://seaborn.pydata.org/}$)$ \\
        Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.
    \end{enumerate}
    
    \item \textbf{Gensim}  $($\url{https://radimrehurek.com/gensim/}$)$ \\
    Gensim is a free python package created for scalable statistical semantics, analysing plain-text documents for semantic structure, retrieving semantically similar documents.
    \begin{enumerate}
    \item \textbf{pyLDAvis} $($\url{https://github.com/bmabey/pyLDAvis}$)$\\
        The pyLDAvis package is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization and can be used in combination with sklearn and gensim.
    \item \textbf{Nltk} $($\url{https://www.nltk.org/}$)$\\
        Nltk is a leading platform for python to work with the human language. The tool can be used to perform every step to transform the human language in a workable form. 
    \end{enumerate}
    
    
    \item \textbf{Conda} $($\url{https://www.anaconda.com/}$)$ \\
    Conda is an open source package and environment management for Python. Conda allows easy setup with out-of-the-box environments for quick testing and removal of environments.
    \begin{enumerate}
        \item \textbf{Jupyter Notebook} $($\url{https://jupyter.org/}$)$ \\
        Jupyter notebook is included in the standard data science conda package. A fast web application used to create documents in Python code to easily share code and visualise data.
        \item \textbf{Python 3.6.X} $($\url{https://www.python.org/}$)$\\
        An user-friendly and elegant programming language which has a great scientific community.
        %, making Python the go to language for data science.
    \end{enumerate}
    
\end{enumerate}