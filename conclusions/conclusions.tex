\chapter{Conclusions} \label{ch:conclusions}
This chapter contains the conclusion in which we will shortly explain the research and derive a conclusion from our experiments in first section. We will answer the research questions and end the chapter with a section about future work. 

\section{Conclusion}
This research has been made with the help of server data of Capgemini. Capgemini provided data of their server datawarehouse for this research. Afterwards we extracted an interesting subset, error data, of data contained in the server logs. Based on earlier research we use the unsupervised machine learning model Latent Dirichlet Allocation(LDA) to discover latent topics in this data set. As such we recognize our dataset as a corpus and the server logs as the documents in this corpus. To achieve this transformation of data to corpus we used some steps.
We preprocessed the data using the standard steps: normalization, stop word removal, tokenization and creating a bag of words matrix (bow). The bow was divided in a train, test and validation set. Furthermore we trained multiple models with different amounts of topics {2-38} with our train and test set. Lastly we evaluate the results of each model using multiple metrics. We evaluate distinctiveness, coherence of each topic using pyLDAvis and the coherence metric Cv. We further use human perception to evaluate the topics with their top terms. The documents are clustered based on their highest probable topic and as such we compare the document distribution based on hard clustering. Finally we use the silhouette coefficient to compare the multiple topics. 

We will answer each research question and end with answering our main research question based on the results.
\begin{itemize}
    \item How do the topic count influence the topic models?
    THe results show different and even contrasting results. We could say with pyLDAvis that the topics have more overlap as topics increase. Coherence stays reasonably the same on lower topic counts. However we noticed a lot of similar top terms in topics in general, increasing overlap of terms as topics increased. The correlation between topic count and topic models show different results for each metric and is inconclusive.
    \item Why is the chosen topic model suitable?
    The optimal topic count is based on the necessary generalisation of the topics to cluster similar documents together. If we were to take coherence as our measure, 11 would be the optimal topic count. However the silhouette value contradicts this value and says everything below 10 topics would be optimal.
    Rather then choosing a single optimal model we recommend using a low amount of topics between 3 and 10 topics based on the pyLDAvis data and coherence score.
    \item What are our findings when applying topic modelling and optimising the model on our data?
    Evaluation is really hard on topic modelling. Especially when the data is domain specific. Server data is not the same as natural language and using topic models will not result in clear topics from each models. However the semantic relation LDA discovers is clusters similar documents, based on hte silhouette values, which would not be easy as human.
\end{itemize}

Leaving us with the research question.
\begin{itemize}
    \item \textbf{\textit{Can we use topic modelling to classify and cluster error messages?}}
\end{itemize}
Based on the results we got with the silhouette values, a low topic counts would be optimal for our model. Furthermore when reviewing the document distribution for topics we see some dominant topics where most of our documents go to. This says more about our data being skewed to certain error messages, which makes it even harder to detect smaller latent topics, which could be more relevant for server logs. The interpretability of topics also leaves much to be desired. When applying the LDA model on similar server logs we recommend having a domain specific expert which might result in better topic evaluation. With the measures we used to evaluate our models, we would not recommend using LDA for classifying and clustering error messages.

\begin{comment}
Making use of the models has shown to cluster new data very well using distance measures like jensen shannon divergence. E We recommend using more simpler techniques on data based on logs. 

We had skewed data from the start, which makes unsupervised clustering with LDA actually less usefull. LDA has more power when used with more full documents, the short nature like tweets doesnt help the. We can't conclude even thought our models performed reasonably based on our own visual evaluation that LDA might be reliable for future use. We propose the usages with LDA with care when applying on small tekst in domain specific use like server logs. The variability of the server logs and our unlabeled nature makes it even harder for applying unsupervised clustering. Althought the premise was interesting. Much improvements can be made in each step based. We recommend a more streamlined pipeline for future use. 
\end{comment}

\section{Discussion}

\subsection {Methodology considerations}
This research took only a small possible path in the finite amount of paths. This section will discuss what future work can be done to improve this research.

\subsection{The reason leading up to LDA}\label{conclusion:discussion}
Although the discussed research has been mainly focused around topic modelling (LDA) on finding similar error logs, the original research question started with a related but different subject. During this section I would like to discuss the original focus of predictive maintenance, as a lot of time has also been put in researching this difficult and challenging task. !!The points that will be discussed are the general application of predictive maintenance and process, the primary tools used, the identified challenges particular to our data set and environment and a recommendation for future work on this subject.!! This part has probably taken 40\% of the total time spent to complete this Thesis, without even being mentioned in the thesis.

\subsubsection{Predictive maintenance}
With the combined application of machine learning and big data companies try to anticipate when machine hardware failure are due to occur. Predicting instead of reacting to problems saves time and money and allows for a better customer experience which can be found in the before mentioned research of Intel \cite{Sipos2014Log-basedMaintenance}\cite{AjayChandramoulyRavindraNarkhedeVijayMungaraGuillermoRueda2013ReducingAnalytics}. It is not hard to imagine why companies like Intel  or Google have already been researching the possibility of big data for this problem. Capgemini came with comparable data to the before mentioned researches to use machine learning to talk about Predictive Maintenance.

\subsubsection{Vrops and syslogs}
The data had not only been system logs but also consisted of vrops logs. In a few words, vrops are unstructured logs created by the virtualisation tools of VMware. The data contained health and performance statistics of their multiple servers. Extracting the values from the vrops databases with the Hadoop framework had great promise to lead to the desired research dataset. Furthermore the syslogs contained an indication of the type of log. Extracting the messages which were severe errors could help label the dataset. Combining the syslogs and vrops seemed the way to go. 

\subsubsection{Unlabelled and unstructured}
Sadly enough, the syslogs were never labelled with their type. This seemed to be a mistake made by their developers when implementing Apache NiFi, which made the data set of the last three years unlabelled and unstructured. This in turn made it impossible to use the vrops values to correctly know when system problems would occur, without having a domain expert available all the time. The only logical step was to find a more mathematical suitable way to label the unlabelled and unstructured data, bringing this research again to the literature study phase after already 3 months of progress.

\subsubsection{Topic modelling}
Going back and forth between different algorithms available, my supervisor finally hinted at Latent Dirichlet Allocation(LDA). Lots of research has been done with LDA. Having already been wasting enough time on searching, this research had to settle for a technique. LDA seemed like an applicable algorithm to this problem based on earlier research. Text mining on logs has been done before, although barely on unlabelled logs. Training LDA on unlabelled data was common, expect that a lot of data contained either longer documents or a less domain specific dataset. In further research I would not recommend LDA for domain specific log research and labelling, unless one has enough time and patience. 

\subsection{Recommendation}
If one is to attempt LDA for further research on logs, be sure to know which results are wished for. Evaluating LDA without a proper desired result might leave general and high valued evaluation techniques useless. In the end human aspect is always involved with LDA.

\section{Future work}
This research leaves a lot of open areas to optimise or further research server logs in general. Experimenting on similar data can be achieved and as such a few points which come to mind when recommending future works are:

\begin{enumerate}
    \item The way documents and corpora are declared. Corpus in our research was 1 day of data on filtered data with the term 'error'. The documents could be an collection of specific server logs instead of 1 log being 1 document, this will increase the document size tremendously and should help LDA perform better.
    \item The pipeline of data extraction. We filtered a lot of server specific features to extract our dataset, which can be a loss of information for LDA. 
    \item Data preprocessing. The steps performed for the data preprocessing can each be 
    \item Exhaustive search. Most parameters were based on best practices from earlier performed, especially by Blei from the original LDA paper. Hyperparameters and topic count can be changed and better set based on the distribution of your documents. This can be quite exhaustive and time consuming, but could be interesting.
    \item The quantity of data. LDA is created to handle large amounts of documents, which we did have but not with the desired document length and variety. Using more varied logs, like informational logs etc could be show better latent topic inference. 
\end{enumerate}
