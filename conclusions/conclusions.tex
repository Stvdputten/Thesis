\chapter{Conclusions} \label{ch:conclusions}
This chapter contains the conclusion in which we will shortly explain the research and derive a conclusion from our experiments. We will answer the research questions and end the chapter with a section discussion and future work.

\section{Conclusion}\label{conclusion:clonclusion}
This research aimed to apply topic modelling for clustering and discovering an optimal model with the help of server data of Capgemini. Capgemini provided data of their server data warehouse for this research. We extracted an subset of data contained in the server logs filtered on the term 'error'. Based on earlier research we use the unsupervised machine learning model Latent Dirichlet Allocation(LDA) to discover latent topics in this data set. As such we recognise our dataset as a corpus and the server logs as the documents in this corpus. To achieve this transformation of data to corpus we used some steps.
We preprocessed the data using the standard steps: normalisation, stop word removal, tokenization and creating a bag of words matrix (bow). The bow was divided in a train, test and held out set. Furthermore we trained multiple models with different amounts of topics {2-38} with our train and test set. Lastly we evaluate the results of each model using multiple metrics. We evaluate distinctiveness, coherence of each topic using pyLDAvis and the coherence metric Cv. We further use human perception to evaluate the topics with their top terms. The documents are clustered based on their highest probable topic and as such we compare the document distribution based on hard clustering. Finally we use the silhouette coefficient to compare the multiple topics. 

We will answer each research question and end with answering our main research question based on the results and the best of our knowledge.

\subsubsection{How does the topic count influence the topic models?}
During this thesis we applied many steps to our data. We chose multiple topic counts using the standard preset the gensim package offered us. For this research we created 25 models with the help of 380000 documents to train and test and the remaining documents to validate the models. Each evaluation measure shows a different side of our models. If we go back to our goal of discovering latent patterns and clustering documents we can explain our results. Our first observation of the pyLDAvis with a topic overview make it clear that the topics are hard to deduce. With pyLDAvis helping us to look fo extra, we see that increasing topics create more overlap of terms. The coherence that we see in Fig \ref{fig:coherence} shows no clear winner, however once again increasing the topic count too much makes the quality even more unpredictable. The contrasting results make it hard to judge these models on quality. Furthermore when comparing our clustered documents in Fig \ref{fig:doc_distr_1-11corpus} and the remaining distribution we can see that the topics generally stay around 2-3 large clusters, interestingly increasing the topic count flattens the distribution of documents, with atleast 1 topic mostly having the largest cluster. The silhouette clearly indicates that higher topic counts have simply to much overlap which makes it not calculable, however lower topic counts show high silhouette scores. The results leave no conclusive decision, but we can say that the higher the topic count the less the quality of the overall clustering. The contrasting results of coherence and pyLDAvis makes it not clear whether interpretability increases with higher topic count for the topics and leaves us inconclusive.

\subsubsection{Why are the chosen topic models suitable?}
In the earlier part of this thesis we analysed multiple researches each with their own implementation of LDA. Our dataset is based on the optimal parameters Blei researched. While we do not have a streaming corpus, the online implementation of LDA that we applied in our thesis allows the models to be updated with new documents. This allows our current models to learn new terms and be able to cluster unseen documents better. This makes our models suitable for future recognition of error logs.
    
\subsubsection{What are our findings when applying topic modelling and optimising the model on our data?}
Evaluation is really hard on topic modelling. Especially when the data is so domain specific. Server data is not the same as natural language and using topic models will not result in clear topics from each model. Our human minds can see some connection between current logs, but we clearly do not posses the skills to interpret the topics objectively. It is simply to hard for a human to deduce the topics with our current dataset and models, which is why a unsupervised machine learning technique as LDA is really optimal to recognise the correlations using semantic analysis.

\noindent Leaving us with the research question.

\subsubsection{\textbf{\textit{Can we use topic modelling to classify and cluster error messages?}}}

When all is said and done. Topic modelling can be used to analyse great deals of documents, discovering latent topics and clustering new unseen data in a similar way. However the application of topic modelling on a dataset which is not even human interpretable makes the results afterwards hard to evaluate. If our application were to be to recognise error messages, this model would be up to the task. The model is even able to be updated. The clusters would be based on the scores of silhouette very good coherent and distinct from other clusters. Which once again beggs the question, can we recognise the topic we put our new document under? It is not very useful to cluster documents together without understanding the topic this document falls under. The interpretability of topics leaves much to be desired. With the measures we used to evaluate our models, we would not recommend using LDA for classifying and clustering error messages. Topic modelling is better of being used on normal human generated corpora.

\section{Discussion}

\subsection {Methodology considerations}
This research took only a small possible path in the finite amount of paths. This section will further explain this path, which led this research to its logical conclusion and a honourable mention to not forget the time spent.

\subsection{The reason leading up to LDA}\label{conclusion:discussion}
Although the discussed research has been mainly focused around topic modelling (LDA) on finding similar error logs, the original research question started with a related but different subject. During this section I would like to discuss the original focus of predictive maintenance, as a lot of time has also been put in researching this difficult and challenging task. 

\subsubsection{Predictive maintenance}
With the combined application of machine learning and big data, companies try to anticipate when machine hardware failure are due to occur. Predicting instead of reacting to problems saves time and money and allows for a better customer experience which can be found in the before mentioned research of Intel \cite{Sipos2014Log-basedMaintenance}\cite{AjayChandramoulyRavindraNarkhedeVijayMungaraGuillermoRueda2013ReducingAnalytics}. It is not hard to imagine why companies like Intel  or Google have already been researching the possibility of big data for this problem. Which brings us to the data acquired for this thesis, originally intended by Capgemini for predictive maintenance. 

\subsubsection{Vrops and syslogs}
The data had not only been system logs but also consisted of vrops logs. In a few words, vrops are unstructured logs created by the virtualisation tools of VMware. The vrops data contained health and performance statistics of their multiple servers. Extracting the values from the vrops databases with the Hadoop framework had great promise to lead to the desired research data necessary for predictive maintenance. Furthermore the syslogs contained an indication of the type of log. Early examination of these logs, brought  messages from all kinds of type of servers. The following step was to understand and extract the data to use in our research.

\subsubsection{Unlabelled and unstructured}
Unluckily we soon enough found the syslogs to be lacking their log type. This seemed to be a mistake made by their developers when implementing their streaming pipeline of server logs, which made the data set of the last three years unclear of their type. This in turn made it impossible to use the vrops values, which could be understood combined with the syslogs, to correctly to know when system problems would occur, without having a consulting a domain expert all the time. The only logical step was to find a more mathematical suitable way to label the unlabelled and unstructured data, bringing this research again to the literature study phase for a new solution.

\subsubsection{Topic modelling}
Going back and forth between different algorithms available, my supervisor finally hinted at Latent Dirichlet Allocation (LDA). Lots of research has been done with LDA. Having already been wasting enough time on searching, this research had to settle for a technique. LDA seemed like an applicable algorithm to this problem based on earlier research. Text mining on logs has been done before, although barely on unlabelled logs. Training LDA on unlabelled data was common, expect that a lot of data contained either longer documents or a less domain specific dataset. 

\subsection{Recommendation}
In further research I would not recommend LDA for domain specific log research and labelling, unless one has enough time and patience and expertise. If one is to attempt LDA for further research on logs, be sure to know which results are wished for. Evaluating LDA without a proper desired result might leave the evaluation process as a tedious phase. 

\section{Future work}
This research leaves a lot of open areas to optimise or further research server logs in general with topic modelling. Experimenting on similar data can be achieved and as such a few points which come to mind when recommending future works are:

\begin{enumerate}
    \item The way documents and corpora are declared. The corpus in our research was 1 day of data, filtered on the term 'error'. The documents could be an collection of specific server instead of 1 log being 1 document, this will increase the document size tremendously and should help LDA perform better.
    \item The pipeline of data extraction. We filtered a lot of server specific features to extract our dataset, which can be a loss of information for LDA. 
    \item Data preprocessing. This is clearly always important when processing the data and can be experimented on in various ways.
    \item Exhaustive search. Most parameters were based on best practices from earlier performed, especially by Blei from the original LDA paper. Hyperparameters and topic count can be changed and better set based on the distribution of your documents. This can be quite exhaustive and time consuming, but could be interesting. Otherwise making use of a super computer speeds up the calculations.
    \item The quality of data. LDA is created to handle large amounts of documents, which we did have but not with the desired document length and variety. Using more varied logs, like informational logs etc could be show better latent topic inference. 
\end{enumerate}
