\chapter{Introduction} \label{ch:introduction}

\section{General introduction}\label{introduction:Generalinformation}
The computers that are used today have been generating data with an explosive rate in the last few years. The data is collected from different sources, transformed and aggregated to be put into a database or data warehouse. The data stays in these databases and has a huge probability to never be used and eventually to be lost. Although companies acknowledge the value of data, it is challenging to make use of said data and even more so to add value to their core business processes. A few challenges brought are due to the volume, velocity and variety of data, to name a few. The recent field of machine learning, or in a more general term data science, embraces data. The exploitation of data has improved the operation of many day to day applications in recent years. 

\setlength{\parindent}{3ex} This thesis is formed with the data Capgemini provided. Capgemini is an international IT consultancy firm that offers its customers IT services. One of these services is the big data lake which allows enormous amounts of data to be stored for further use. This data lake is built with the open source Hadoop framework. Capgemini allowed us access to their big data lake containing millions of server logs of their customers and systems. The server logs are used to find explanation for whenever server failure occurs. Manually inspecting server logs is time consuming and is costly as only domain experts understand the server logs. This brings us to the request of the company to help them get more use out of their data. 

With enough creativity and time we would be able to use such a source of data to infinite use cases. Sadly, such extensive research is not possible. The research started with the analyses of the data and eventually led to applying topic modelling. Topic modelling is a form of unsupervised machine learning. Topic modelling can be described as the extraction of latent patterns (hidden topics) from data through semantic analysis. Readers who are further interested why topic modelling has been chosen, are encouraged to read section \ref{conclusion:discussion}. 

\begin{comment}
--todo--
The intention of this research started with analysing e system logs to help create a model for predicting hardware and software failure for maintenance and automatic self-healing. The huge amount of system logs available from a variety of systems brought the question how to analyse and make use of the logs to predict hardware and software failure.

Current research of big data makes this a suitable problem to solve through recent machine learning techniques. 
During the time spent on this research challenges were met and identified for realising this goal and ended with the usage of Natural Language Processing (NLP) and unsupervised learning.  The untapped amount of raw data makes it possible for many more application, but in further paragraphs it will be made clear why NLP was chosen and what more could be applied on this Big data problem.

\end{comment}
 
\section{Problem statement}\label{introduction:Motivation}
Using the data to extract value is too general for a problem statement and that is why we need to specify the exact motivation and goals of this thesis. The only premise is the application of machine learning on the data. The general steps of research are exploration of the data, to see what machine learning tool is applicable and to apply and optimise the model generated (if possible). The server logs generally consist of textual messages. The messages describe the servers and range from simple informational messages to severe warning messages where user intervention is necessary. The only problem with the data is that it does not make a distinction between the types of messages. The domain expert might know which messages to search for when the servers fail, but with millions of logs the domain expert cannot be aware of all the necessary logs. Especially when the logs contains no label to indicate their type. This is one of the reasons why we propose topic modelling. Topic modelling serves as a statistical technique mostly used to reduce the dimensions of data, but can also be used to cluster similar messages.

\begin{comment}

Topic modelling is a hot topic in data science. System logs are used as source for the detection of problems in large computer systems. While domain experts can be used to detect and fix the problems detected, this can be difficult and time consuming. Machine learning techniques like topic modelling make it possible to develop models to extract these latent patterns from these system logs. While topic modelling is normally used in for large text corpera, recent research in the field of short text clustering and twitter tweets clustering are similar enough to by applicable for system logs. An interesting application which has not yet been touched a lot through unsupervised machine learning techniques.

\end{comment}


\section{Research question}\label{introduction:Researchquestion}
The main goal of this research is to apply topic modelling to cluster these messages in distinct groups, we comparing these models and evaluating which model is most suitable.

\noindent We propose the following research question: 
\begin{itemize}
    \item \textbf{\textit{Can we use topic modelling to classify and cluster error messages?}}
\end{itemize}

\noindent To help us answer this questions we will distinguish the research in three subquestions:
\begin{itemize}
    \item How do the topic count influence the topic models?
    \item Why is the chosen topic model suitable?
    \item What are our findings when applying topic modelling and optimising the model on our data?
\end{itemize}

\begin{comment}
What are the optimal parameters?

What are the pro's and con's of using LDA?

subquestions:
Why is LDA suitable for this type of data 

How do the parameters influence the models performance?

What are the pro's and con's of using LDA?

What other methods are available to solve this error clustering?


\end{comment}


\begin{comment}

How do we define similair messages?

Can we cluster these error messages to find patterns?

Wat ik zou verwachten in je scriptie zijn de volgende topics
•	Wat is het probleem (de probleemstelling)?
•	Welke mogelijkheden zijn er om dit probleem op te lossen?
•	Welke methode heb je gekozen, en vooral uitleggen waarom deze methode volgens jou de beste is?
•	Hoe heb je vastgesteld dat de gekozen oplossing de beste is?
•	Wat is de uitkomst?
•	Wat zijn de voor en nadelen van het gekozen model, wat zijn de beperkingen, wat is de optimale modelering en waarom?
Zijn deze onderwerpen voldoende afgedicht in onderstaande structuur?

Ik ben veel meer geïnteresseerd in de onderbouwing:
•	Wat is de (onze) probleem omschrijving
Finding structures in syslogs to cluster undiscovered syslogs with errors
•	Waarom kies je LDA om dit probleem te lijf te gaan
o	Pro’s / con’s
o	Alternatieven
•	Hoe moet LDA gebruikt worden
o	Welke specifieke tuning heb je gebruikt
o	Hoe beinvloed de aanpassingen van parameters de uitkomst


\textbf{Can we make a reliable model for error detection in system logs?}

 \end{comment}
 
\section{Thesis Overview} \label{introduction:thesisoverview}
The remaining thesis is structured as follows:

\setlength{\parindent}{3ex} \textbf{Chapter \ref{ch:research}}: Two examples of previous research are discussed. Furthermore, previous works on feature extraction from server logs is described. The building blocks used to perform this research are also described here.


\textbf{Chapter \ref{ch:theory}}: This chapter gives the necessary theoretical background. The first section explains machine learning. The second section is about topic modelling. The last section is contains definitions of the Latent Dirichlet Allocation (LDA) model. 


\begin{comment}
natural language processing, feature extraction, types of system logs, the application of machine learning, general machine learning knowledge 
\end{comment}

\textbf{Chapter \ref{ch:methodology}}: This chapter described the process of the data and the steps that are taken to prepare the data. The second section is about the 4 evaluation metrics for the model.

\textbf{Chapter \ref{ch:results}}: This chapter shows the results from the application of the LDA model on the data set. The performance of our model based on the 4 metrics, which measure the quality of the topic model showing different scores depending on the given parameters.

\textbf{Chapter \ref{ch:conclusions}}: The conclusion based on the found results. The possible future work, discussion and our final recommendation and thoughts about this research.

\textbf{Chapter \ref{ch:appendices}}: The appendix contains examples and figures of the data Capgemini provided and the data that has been preprocessed. The pyLDAvis figures generated of our models. The topics infered for different topic counts. Lastly the documents distribution of each topic in each of our models.

\begin{comment}
It is recommended to end the introduction with an overview of the thesis. This chapter contains the introduction; Chapter~\ref{ch:definitions} includes the definitions; Chapter~\ref{ch:relatedwork} discusses related work; Chapter~\ref{ch:evaluation} evaluates the contributions; Chapter~\ref{ch:conclusions} concludes.

Also make a nice sentence with ``bachelor thesis'', LIACS and the names of the supervisors.

\end{comment}