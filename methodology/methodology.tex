\chapter{Methodology}  \label{ch:methodology}

\begin{comment}
Why did I choose my method
\end{comment}

\section{Text preprocessing}
In this section we'll talk about the input dataset, the reason why we selected it and how it got processed for our model. Preprocessing involves tokenization and stop word removal discussed in section \ref{tokenization} - \ref{bagow}. Preprocessing is important and if done wrong can impact the final results. This can be related to the garbage in, garbage out principle in computer science, where flawed input brings nonsense output. 

\subsection{Dataset}
The dataset used in this research is provided by Capgemini containing syslogs from various servers. The syslogs contains event logs from different servers provided to their customers and internal staff. The event logs used on their servers were extracted from different operating systems, ranging from the year 2015 to current day. The size of one day of data can easily range into the 20 - 40 million event logs. Due to the size and complexity and computation time of the dataset, this research contains 1 day of data with the word error to ensure it's associaten with erorrs.
The data will be referred as the corpus and the system logs as documents throughout this research. The corpus contains 426928 records and has messages consisting of small twitter like sizes. The documents has a few features like hostname, severity, port, priority, valid, protocol, body. The features contain little information and will not be used, only the body feature has been extracted which contains the event log.


\begin{table}[h]
\centering
 \begin{tabular}{|l|l|l|l|} 
 \hline
 Collection & Documents & Words & Vocabulary  \\ [0.5ex] 
 \hline\hline
 Complete dataset & 18369485 & 310 million & N.A.  \\ 
 Error dataset & 426905 & 3428621 & 1694 \\
 \hline
 \end{tabular}
\caption{Statistics about the dataset}
\label{tab:table1}
\end{table}


\subsection{Tokenization}\label{tokenization}
The first step for our preprocessing is the cleaning up of the documents. 
In the preprocessing phase the document needs to be cleaned up and normalised to ensure consistency in the input data.

The content of the document consists of the error message and specific user and server information. An algorithm had to be written to remove the unnecessary information from the dataset. This extracted the message from document and removed the specific users and server information from the message. Afterwards the message got normalised by removing unnecessary punctuation and removing uppercase.

\subsection{Stop word}\label{stop_words}
LDA assumes that each word is equally important, which is simply deduced to not be true from a logical and linguistic point of few. Words such as 'the' and 'a\\an' are not important, generic English words can be removed to only keep the most distinguishable words left. The provided tools in sklearn provide a standard tokenization option with stop words from the English vocabulary. This tool removes remaining ambigious words like single character words and leaves the most important words left that are specific to the error message.

\subsection{Bag of words} \label{bagow}
The text preprocessing in this research makes usage of the bag-of-words representation as LDA makes the assumption that the order of the words does not matter \cite{Blei2010}. Bag of word counts the words that appeared in the document and represent the words in a document as a term frequency matrix. The bag of words representation of a document doesn't take in the order or semantic structure in a document, but LDA discovers these semantic structures itself. 

\subsection{Dimensionality reduction}
The reason LDA is widely applied for text clustering is because LDA actually reduces the dimension, reducing the computation time. Reducing the dimension with LDA allows the usage of Jensen-Shannon distance to measure text similarity, which we discuss in section \ref{research:jsdivergence}. The generated output is also suitable for clustering, but the output can also be interpreted as clusters.

\begin{comment}

\section{Syslog} 
Syslogs are textual messages \cite{Stearley2004TowardsSyslogs} which contain the messages provided by the systems.


\section{Challenges}

\subsection{Natural Language Processing}


\subsection{Semantics}


\subsection{Sentiment analysis}


\subsection{Extract, Transform, Load}

\end{comment}