\chapter{Methodology}  \label{ch:methodology}

In this chapter, the dataset and methodology are described. Section \ref{methodology:dataset} examines the raw data. In Section \ref{methodology:Text preprocessing} the steps of transforming the data is described. Section \ref{methodology:explo&visu} is a brief look at the data. Lastly, Section \ref{methodology:evaluation measures} describes our measures to evaluate our models.

\section{Data collection}
The original data is collected using the Hadoop framework tools named in Section \ref{research:buildingblocks}. HDFS is a storage system which collects the different servers logs from different type of servers. With the help of the available tools we extract the unstructured data and use the existing library to structure the data. PySpark allows quick in memory computation to transform and slice the data further. Which eventually allows us to transform the data to the friendlier pandas dataframe and make local computations possible.  
 
\section{Dataset}\label{methodology:dataset}
The dataset used in this research is provided by Capgemini containing syslogs from various servers, see Appendix A \ref{appendices:serverdata}. The syslogs contains server logs from different servers provided to their customers and internal staff. The event logs used on their servers were extracted from different operating systems, ranging from the year 2015 to current day. The size of one day of data can easily range into the 20 - 40 million server logs. Due to the size and complexity and computation time of the dataset and the focus on discovering patterns in error logs in the unlabelled data, we extracted the data that contained the word "error". 

The filtered dataset has been transformed to a more suitable pandas dataframe in Table \ref{tab:tableerror}. The column syslog.body contains textual messages displaying the messages from the server. Manual inspection has shown that the contextual data sent by the server is displayed after the square bracket. We simply filtered the data away before the square bracket using regular expressions as this contains low to non-existent value for our research. 
With that said the example below in Table \ref{tab:tablemessage} will be left with: \textit{RestClient: HTTP request for url /migrate/ping failed with error code 12029 (source: 5).} 

\begin{table}[h]
\centering
    \begin{tabular}{|l|} 
    \hline
    - - - [Originator@6876 eventid="300" keywords="Classic" level="Error" channel="Application" vmw\_host \\
    ="vbk-dca-esx-020.piv.local" vmw\_vcenter\_id="7CE97301-0536-455B-9466-475070F453E3" vmw\_vcenter= \\ 
    "PRD vSphere Environment" providername="Engine"  vmw\_vr\_ops\_id="58640a59-4d7f-42fe-9ff1-d54fa1f595f6" \\ vmw\_cluster="css01-piv-01" vmw\_datacenter="Pivotal-DCA" task="None"  vmw\_object\_id="vm-6643" \\ eventrecordid="64509969"] 
    RestClient: HTTP request for url /migrate/ping failed with error code 12029 \\ 
    (source: 5) \\
    \hline
    \end{tabular}
\caption{Full length syslog.body message}
\label{tab:tablemessage}
\end{table}


\begin{table}[h]
\centering
 \begin{tabular}{|l|l|l|} 
 \hline
  & hostname & uuid \\ [0.5ex] 
 \hline
 0 & piv-prd-os-362.iddsaprod.lan & a6e7c3d3-c8b4-4cc4-835f-cf6643b76622 \\
 1 & piv-prd-os-362.iddsaprod.lan &	ae4ad420-5120-4345-8c43-11b3db6cae65 \\
 2 & vbk-dca-esx-033.piv.local &	1797b5ff-d4f7-43dd-b99c-64ae6e688fdf \\
 3 & piv-prd-os-362.iddsaprod.lan &	a47434ef-07ba-47a5-85d6-e2927065b4c1 \\
 4 & piv-prd-os-362.iddsaprod.lan &	5f9c1433-d3a5-4f15-bfa4-50ffdc913a0a \\
 \hline\hline
  & syslog.body	& timestamp \\ 
 \hline
 0 & - - - [Originator@6876 eventid="300" keywords=...  & 2017-05-01T21:04:20.0Z \\
 1 & - - - [Originator@6876 eventid="300" keywords=...	& 2017-05-01T21:04:19.0Z \\
 2 & sfcb-CIMXML-Processor 7620520 - [Originator@68...	& 2017-05-01T21:04:20.557Z \\
 3 & - - - [Originator@6876 eventid="300" keywords=...	& 2017-05-01T21:04:20.0Z \\
 4 & - - - [Originator@6876 eventid="300" keywords=...	& 2017-05-01T21:04:20.0Z \\
 \hline
 \end{tabular}
\caption{The local dataframe}
\label{tab:tableerror}
\end{table}

Now that essential part has been extracted from our data, we discard the remainder and will continue the process in the next section. 


\section{Data preprocessing}\label{methodology:Text preprocessing}
Our goal now is to prepare the data such that our model can accept the data. The data needs to be converted to the Bag of Words representation. Preprocessing involves normalization, tokenization and stop word removal discussed in Section \ref{methodology:normalization} - \ref{methodology:bagow}. Preprocessing is important and can greatly influence the final results. This can be related to the garbage in, garbage out principle in computer science, where flawed input brings nonsense output. Following the same principles laid in the Section \ref{research:featureextraction} we will walk through every step. This said, we can only use the data contained in the dataset with feature 'syslog.body'. This feature contains the messages. 

The preprocessing of our data is defined in 4 steps:
\begin{enumerate}
    \item Normalization
    \item Stop Words
    \item Tokenization
    \item Bag of Words
\end{enumerate}

Before such steps are taken we have an overview of our current dataset and complete dataset mentioned in Table \ref{tab:tablestatistics}. The corpus contains 426905 records and has messages consisting of small twitter like sizes. The complete dataset has documents which contain a few columns like hostname, severity, port, priority, valid, protocol, body. The features contain little information and will not be used, only the body feature has been extracted which contains the textual message of the syslog. Furthermore, we assume that our syslogs are normal textual messages and our Bag of Words matrix will only contain 1-gram words, unless we state otherwise.  
The data will be referred as the corpus and the syslogs as documents in the remaining paper. 


\begin{table}[h]
\centering
 \begin{tabular}{|l|l|l|l|} 
 \hline
 Collection & Documents & Words & Vocabulary  \\ [0.5ex] 
 \hline\hline
 Complete dataset & 18369485 & 310 million & N.A.  \\ 
 Error dataset & 426905 & 3428621 & 1694 \\
 \hline
 \end{tabular}
\caption{Statistics about the dataset before processing}
\label{tab:tablestatistics}
\end{table}

\subsection{Normalization}\label{methodology:normalization}
In the text mining world, we define normalization as follows: \\

\theoremstyle{definition} 
\begin{definition}{Normalization} 
\\\textit{Text normalization is the process of transforming text into a single canonical form that it might not have had before \cite{Textnorm}.}
\end{definition}

Normalization in normal documents is simply done by removing the punctuation marks and the lowercase of each word. The nature of our server logs does not allow that. The message concealed in Table \ref{tab:tablemessage} has an error code and extra information between the parenthesis's. We choose to keep the additional information closed between the parenthesis's, using python to write a custom function. The next steps are removing the digits and remaining punctual marks, lowercase every word and remove the remaining unnecessary whitespace. 

\subsection{Stop word}\label{methodology:stop_words}
The second step for our preprocessing is removing Stop Words. Once again Stop Words are:

\theoremstyle{definition} 
\begin{definition}{Stop Words} 
\\\textit{Stop Words are words which are filtered out before or after processing of natural language data \cite{Anand2011}.}
\end{definition}

LDA assumes that each word is equally important. We assume that each word is not equally important which is why we remove the unimportant words, called stop words. Words such as 'the', 'a' and 'an' are not important, generic English words can be removed to only keep the most distinguishable words left. 
The tools in NLTK provide a standard tokenization option with stop words from the English vocabulary. This tool removes remaining ambiguous words and leaves the most important words left that are specific to the error message. 

\subsection{Tokenization}\label{methodology:tokenization}
The third step in our preprocessing is the parsing of the documents. After the stop words have been removed, we are left with documents containing highly specific words. The tokenization process takes care of the remaining text and parses the documents. Further vectorization is needed to transform the tokens into the Bag of Words representation.

\subsection{Bag of Words} \label{methodology:bagow}
The text preprocessing in this research makes usage of the Bag of Words representation as LDA makes the assumption that the order of the words does not matter \cite{Blei2010}. Bag of Words counts the words that appeared in the document and represent the words in a document as a term-frequency matrix (tf-matrix). The Bag of Words representation of a document does not take in the order or semantic structure in a document, but LDA discovers these semantic structures itself. 

\begin{comment}
An other notable representation is the \textit{term frequency inverse document matrix} (tf-idm) in topic modelling. Tf-idm increases the weight terms with less frequency have. This makes tf-idm more appealing when assuming infrequent terms are important. Because tf-idm already reduces the dimension of the terms, lda cannot use this matrix as input. 
\end{comment}

\section{Data exploration \& Visualisation} \label{methodology:explo&visu}
In this section we will further explore our resulting tf-matrix and mention the model parameters.

\subsection{Term frequency matrix}
The remaining tf-matrix will be once again filtered. The words which occur in more than 90\% of the occurring documents and words appearing less than three times will be removed. Having words that are too frequent have no to little information gain and terms that occur less than two times will not be relevant enough to keep. Our resulting words with their respective frequency is displayed in Fig \ref{fig:corpustermfreq}. This leaves our dictionary with a vocabulary of 1570 words in total.

 
 
\begin{figure}[h]
 
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{methodology/Wordcount.png} 
\caption{Atleast 1000 times}
\label{fig:Termfreq}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=0.9\linewidth, height=5cm]{methodology/Wordcount_filt.png}
\caption{More than 1000 and less than 250000 times}
\label{fig:Termfreq_filt}
\end{subfigure}
 
\caption{Words and counts appearing in the corpus}
\label{fig:corpustermfreq}
\end{figure}

\begin{comment}


\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{methodology/Wordcount.png}
    
    
\end{figure}

Fig \ref{Termfrey} clearly displays a slightly vertical term curve at the starts, making us assume that these words are strongly correlated.
As each document is a form of error we can see that the most common terms in Fig \ref{fig:Termfreq_filt} are user, information, retrieving.  If we remove the most common words we are left with Fig \ref{fig:corpustermfreq}, which better displays term less frequent. 

!!TODO

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{methodology/Wordcount_filt.png}
    
\end{figure}
\end{comment}


\subsection{Splitting the data in Train Test and Held out} \label{methodology:traintestheldout}
The data is shuffled using sklearns build in shuffle. WE use 90\% of the data for out train and test data and the remaining 10\% for held out data. The size our train test is 384215 documents and held out is 42690 documents.

\subsection{Dimensionality reduction} \label{methodology:dimreduction}
The reason LDA is widely applied for text clustering is because LDA actually reduces the dimension, reducing the computation time. LDA reduces the dimension of a document (in our case with the shape (doc, term) to a (doc,topic) shape. The generated output is soft clustered, but will be also hard clustered. Each topic in the (doc, topic) output corresponds to the probability the documents belongs to that topic.


\begin{comment}
\section{Syslog}\label{methodology:syslog}
Syslogs are textual messages \cite{Stearley2004TowardsSyslogs} which contain the messages provided by the systems.
\section{Challenges}
\subsection{Natural Language Processing}
\subsection{Semantics}
\subsection{Sentiment analysis}
\subsection{Extract, Transform, Load}
\end{comment}

\subsection{Model Building} \label{methodology:model_building}
Building further on the research done on LDA, the before mentioned research in Chapter \ref{ch:research} and more recent research with online LDA. Table \ref{tab:tableparset} displays the parameters set to test our model. The named parameters are explained in more detail on the gensim webpage in Section \ref{research:buildingblocks}. 

\begin{table}[h]
\centering
 \begin{tabular}{|l|l| l|} 
 \hline
 Parameter & value & description \\ 
 \hline
 \hline
 $\alpha$ (alpha) & 1.0/Number of topics & A prior belief of each topic \\ 
 $\beta$ (beta) & 0.1 & a prior belief of each word  \\
 $\kappa$ (kappa) & 0.5 & weight of word remembered each topic \\
 $\tau$ (tau) & 64 & weight of topic remembered each document\\
 num topics & K{2-38}& Number of latent topics to be extracted\\
 chunksize & 2000 & Number of documents in each batch\\
 passes & 1 & number of passes through the corpus\\
 update every & 1 & sets the model to online version\\
 iteration & 50 & maximum number of passes through the corpus when infering corpus\\
 scorer & perplexity & uses perplexity to train the model on held out test data\\
 gamma threshold & 0.0001 & minimum chance in document needed to continue iterating\\
 eval every & 10 & Estimates perplexity after update\\
 id2word & dictionary & used to map the Id to word \\
 \hline
 \end{tabular}
\caption{Parameter settings}
\label{tab:tableparset}
\end{table}

\section{Model evaluation}\label{methodology:evaluation measures}
Each step in our process needs to be evaluated. Some evaluation metrics are already included and applied by our models. Others are common practice and applied to our results. The following metrics to evaluate are coherence, perplexity, silhouette coefficient, human readability and Jensen shannon.

\subsection{Coherence}\label{methodology:coherence c_v}
We use coherence to evaluate the topics coherence. Coherence is the distinctiveness of each topic. This can be achieved using a human interpretability score \cite{Chang2009} or other measures. In a research conducted in 2015 we find that the Cv measure outperforms other topic cohesion measures. The measure correlates well with human interpretability \cite{Roder2015}.

\subsection{Perplexity}\label{methodology:perplexity}
In the original paper Blei introduces a general model evaluation metric \cite{Blei2003} to compare topic models. Perplexity can be used to compare the generalisation of a model on new unlabelled dataset.

\[
   \mathlarger{perplexity(\textbf{D}_{test}) = \exp{\Bigg \{ -\frac{\sum{}_{d=1}^{M}logp(\textbf{w}_d)}{\sum{}_{d=1}^{M}N_d} \Bigg \}}}
\]

Perplexity shows the perplexity on the test set of held out documents $\textbf{D}$. The nominator shows the sum in  corpus $M$ with document $d$, where the likelihood of each word in d is computed. The denominator consists of the count of words $N$ in document $d$. The lower the perplexity score the better a model generalises.

\subsection{Silhouette coefficient} \label{methodology:silhouette}
The silhouette is used to measure between the cohesion and the separation of intra-clusters. In our model this measures the mean intra-cluster distance for each document and compares distance to the nearest-cluster distance.

\[
   \mathlarger{s(i) = \frac{b(i) - a(i)}{\max{\{ a(i), b(i)}\} } }
\]

Where $s_i$ is the silhouette of sample $i$ in the cluster. $a_i$ is the average distance for $i$ from all the objects in the cluster and $b_i$ the distance of $i$ from the closest cluster $b$ not containing $i$. 

\[
\mathlarger{-1 \leq s \le 1}
\]

The value of $s$ will be contained between $-1$ and $1$. If $s(i) = 1$ then we can say that the distance $i$  is a lot less in its own cluster then the nearest other cluster. If we take $s(i) = -1$ then the similarity of $i$ is higher in the other nearest cluster then its current cluster \cite{Rousseeuw1987Silhouettes:Analysis}.. Commonly used with cosine for document cluster evaluation.


\subsection{Jensen-Shannon divergence and KL-divergence} \label{methodology:jsdivergence}
The Kullback Leiber divergence was introduced to measure the density between two distributions \cite{Hershey2007ApproximatingModels}. Based upon this important and popular measure the Jensen-Shannon divergence (JSD) was introduced \cite{Fuglede2004Jensen-ShannonEmbedding}. Which is better used to measure similarity between two text documents based on their probability distributions.

\[
\mathlarger{JDS(P||Q) = \frac{1}{2}D(P||M) + \frac{1}{2}D(Q||M)}
\]

Where $P$ and $Q$ denote a probability distribution and $M$ the set of probability distributions. Taking the square root of JSD makes it truly a metric which can be used to measure similarity \cite{Huang2008}. The metric is especially useful to find the distinctiveness and cohesion between topics.

\subsection{Human perception}\label{methodology:humanperception}
Although LDA can be used to find latent patterns, explore, tag recommend in a document corpus the final result of topics do not necessarily match up with the human expectation of a topic. Especially in an unsupervised learning model with only mathematical measures\cite{Towne2016MeasuringPerception}. The reason of this paragraph is to make readers aware that the suitability of a model in an unsupervised learning and NLP environment still need support of a human factor. Research from Chang  et al. \cite{Chang2009} and Blei et al. \cite{Chaney2012VisualizingModels.} provide more in depth research in this topic.  

With that in mind, the highly dimensional LDA is actually suitable in contrast to most machine learning algorithms to be evaluated using visual tools. One such tool is LDAvis, a tool that got developed in R and D3 \cite{Sievert2014}. LDAvis is a web-based interactive visual of the topics on a fitted LDA model. The multidimensional LDA is scaled to two dimensions, making it possible to visually see the distance between topics and quickly determine their distinctiveness. Simultaneously the visual tool shows the relevance of each term in their selected topic, based on their exclusiveness and occurs within that topic compared to different topics. The implementation that we use is created with python and is named pyLDAvis, see Section \ref{research:buildingblocks}. 

\begin{comment}
    \subsection{Parameter tuning}\label{methodology:parameter tuning}
\end{comment}
The evaluation metrics will be applied on the models. Based on the exploration of the data we will also choose the optimal number of topics. In the following Chapter \ref{ch:results} the results are shown. 